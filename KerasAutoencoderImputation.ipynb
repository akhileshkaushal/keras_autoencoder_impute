{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16240, 126)\n",
      "    Gene  GSM2069305  GSM2069306  GSM2069307  GSM2069308  GSM2069309  \\\n",
      "0      1    0.316824    0.252532    0.243955    0.234106    0.292485   \n",
      "1     10         NaN   -0.138681   -0.131388   -0.154578   -0.133985   \n",
      "2    100    2.258508    2.220878    2.292011    2.221837    2.408065   \n",
      "3   1000    0.015773    0.002357   -0.066824   -0.055737    0.070111   \n",
      "4  10000    0.083632    0.072687    0.073163   -0.005703    0.042765   \n",
      "\n",
      "   GSM2069310  GSM2069311  GSM2069312  GSM2069313     ...      GSM2069420  \\\n",
      "0    0.268240    0.321052    0.270014    0.269687     ...        0.372067   \n",
      "1   -0.073763   -0.108244   -0.101171   -0.075025     ...        0.565097   \n",
      "2    2.321494    2.398902    2.409410    2.167130     ...        1.911440   \n",
      "3    0.058310   -0.038059   -0.017265   -0.116453     ...       -0.069461   \n",
      "4    0.041141    0.025783    0.041102    0.042092     ...        0.224562   \n",
      "\n",
      "   GSM2069421  GSM2069422  GSM2069423  GSM2069424  GSM2069425  GSM2069426  \\\n",
      "0    0.675189    0.764933    0.413545    0.830016    1.040675    0.517280   \n",
      "1    0.806116    0.610597    0.329307    1.401139    1.096701    0.591318   \n",
      "2    2.111528    1.735172    1.907772    2.218028    1.950766    2.154102   \n",
      "3   -0.143092   -0.127216   -0.099226   -0.198093   -0.163434    0.145684   \n",
      "4    0.089299    0.266195    0.279413   -0.079563    0.017466    0.145684   \n",
      "\n",
      "   GSM2069427  GSM2069428  GSM2069429  \n",
      "0    1.115917    0.814627    0.520985  \n",
      "1    1.455757    0.679422    0.554491  \n",
      "2    2.001419    1.816722    2.059040  \n",
      "3   -0.153950   -0.094815   -0.115838  \n",
      "4   -0.068360    0.098052    0.175374  \n",
      "\n",
      "[5 rows x 126 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "gene_exp = pd.read_csv('/data/keras_autoencoder_imputation/E-GEOD-78193.pcl',\n",
    "                       delimiter='\\t')\n",
    "print(gene_exp.shape)\n",
    "print(gene_exp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.4673     \n",
      "Epoch 2/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.2347     \n",
      "Epoch 3/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.2074     \n",
      "Epoch 4/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1988     \n",
      "Epoch 5/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1966     \n",
      "Epoch 6/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1957     \n",
      "Epoch 7/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1950     \n",
      "Epoch 8/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1946     \n",
      "Epoch 9/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1942     \n",
      "Epoch 10/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1939     \n",
      "Epoch 11/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1936     \n",
      "Epoch 12/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1934     \n",
      "Epoch 13/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1932     \n",
      "Epoch 14/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1930     \n",
      "Epoch 15/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1927     \n",
      "Epoch 16/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1924     \n",
      "Epoch 17/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1922     \n",
      "Epoch 18/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1919     \n",
      "Epoch 19/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1916     \n",
      "Epoch 20/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1914     \n",
      "Epoch 21/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1911     \n",
      "Epoch 22/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1908     \n",
      "Epoch 23/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1907     \n",
      "Epoch 24/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1904     \n",
      "Epoch 25/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1902     \n",
      "Epoch 26/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1900     \n",
      "Epoch 27/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1899     \n",
      "Epoch 28/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1897     \n",
      "Epoch 29/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1896     \n",
      "Epoch 30/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1894     \n",
      "Epoch 31/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1892     \n",
      "Epoch 32/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1891     \n",
      "Epoch 33/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1890     \n",
      "Epoch 34/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1889     \n",
      "Epoch 35/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1888     \n",
      "Epoch 36/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1886     \n",
      "Epoch 37/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1885     \n",
      "Epoch 38/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1884     \n",
      "Epoch 39/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1884     \n",
      "Epoch 40/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1882     \n",
      "Epoch 41/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1882     \n",
      "Epoch 42/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1881     \n",
      "Epoch 43/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1880     \n",
      "Epoch 44/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1879     \n",
      "Epoch 45/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1879     \n",
      "Epoch 46/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1877     \n",
      "Epoch 47/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1877     \n",
      "Epoch 48/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1877     \n",
      "Epoch 49/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1876     \n",
      "Epoch 50/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1875     \n",
      "Epoch 51/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1875     \n",
      "Epoch 52/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1874     \n",
      "Epoch 53/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1873     \n",
      "Epoch 54/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1873     \n",
      "Epoch 55/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1872     \n",
      "Epoch 56/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1872     \n",
      "Epoch 57/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1872     \n",
      "Epoch 58/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1871     \n",
      "Epoch 59/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1871     \n",
      "Epoch 60/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1871     \n",
      "Epoch 61/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1869     \n",
      "Epoch 62/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1870     \n",
      "Epoch 63/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1869     \n",
      "Epoch 64/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1869     \n",
      "Epoch 65/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1869     \n",
      "Epoch 66/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1868     \n",
      "Epoch 67/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1868     \n",
      "Epoch 68/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1868     \n",
      "Epoch 69/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1868     \n",
      "Epoch 70/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1867     \n",
      "Epoch 71/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1867     \n",
      "Epoch 72/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1867     \n",
      "Epoch 73/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1868     \n",
      "Epoch 74/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1866     \n",
      "Epoch 75/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1866     \n",
      "Epoch 76/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1866     \n",
      "Epoch 77/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1866     \n",
      "Epoch 78/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1866     \n",
      "Epoch 79/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1866     \n",
      "Epoch 80/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1867     \n",
      "Epoch 81/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1865     \n",
      "Epoch 82/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1866     \n",
      "Epoch 83/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1865     \n",
      "Epoch 84/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1865     \n",
      "Epoch 85/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1865     \n",
      "Epoch 86/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1864     \n",
      "Epoch 87/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1865     \n",
      "Epoch 88/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1865     \n",
      "Epoch 89/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1864     \n",
      "Epoch 90/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1864     \n",
      "Epoch 91/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1864     \n",
      "Epoch 92/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1865     \n",
      "Epoch 93/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1864     \n",
      "Epoch 94/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1864     \n",
      "Epoch 95/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1864     \n",
      "Epoch 96/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1863     \n",
      "Epoch 97/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1863     \n",
      "Epoch 98/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1864     \n",
      "Epoch 99/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1863     \n",
      "Epoch 100/100\n",
      "16240/16240 [==============================] - 0s - loss: 0.1863     \n"
     ]
    }
   ],
   "source": [
    "# First just replace nan with 0 (similar to denoising autoencoder)\n",
    "# it may be helpful to actually add noise to other columns as well....\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "x = gene_exp.as_matrix()[:, 1:]\n",
    "x_train = np.nan_to_num(x)\n",
    "\n",
    "input_exp = Input(shape=(125,))\n",
    "encoded = Dense(64, activation='relu')(input_exp)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(125, activation='tanh')(decoded)\n",
    "\n",
    "autoencoder = Model(input_exp, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='mse')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                batch_size=256,\n",
    "                epochs=100,\n",
    "                shuffle=True)\n",
    "\n",
    "x_run = x\n",
    "ae_out = autoencoder.predict(x_run)\n",
    "\n",
    "missing_mask = np.isnan(x)\n",
    "x[missing_mask] = ae_out[missing_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "16240/16240 [==============================] - 0s - loss: 0.4803     \n",
      "Epoch 2/2\n",
      "16240/16240 [==============================] - 0s - loss: 0.2413     \n",
      "MSE after 2 epochs:\n",
      "0.0200173653288\n",
      "\n",
      "Epoch 1/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.2101     \n",
      "Epoch 2/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.2029     \n",
      "Epoch 3/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.2013     \n",
      "Epoch 4/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.2003     \n",
      "Epoch 5/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1997     \n",
      "Epoch 6/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1992     \n",
      "Epoch 7/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1988     \n",
      "Epoch 8/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1985     \n",
      "Epoch 9/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1982     \n",
      "Epoch 10/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1980     \n",
      "Epoch 11/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1977     \n",
      "Epoch 12/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1975     \n",
      "Epoch 13/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1973     \n",
      "Epoch 14/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1971     \n",
      "Epoch 15/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1969     \n",
      "Epoch 16/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1967     \n",
      "Epoch 17/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1965     \n",
      "Epoch 18/18\n",
      "16240/16240 [==============================] - 0s - loss: 0.1963     \n",
      "MSE after 20 epochs:\n",
      "0.00461483333107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test with data MCAR\n",
    "md_per = 0.02\n",
    "\n",
    "x = gene_exp.as_matrix()[:, 1:]\n",
    "orig_present = ~np.isnan(x)\n",
    "\n",
    "for row in x:\n",
    "    rand_missing = np.random.randint(0, x.shape[1], int(np.ceil(x.shape[1]*md_per)))\n",
    "    row[rand_missing] = np.nan\n",
    "    \n",
    "x_train = np.nan_to_num(x)\n",
    "missing_mask = np.isnan(x)\n",
    "\n",
    "input_exp = Input(shape=(125,))\n",
    "encoded = Dense(64, activation='relu')(input_exp)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(125, activation='tanh')(decoded)\n",
    "\n",
    "autoencoder = Model(input_exp, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='mse')\n",
    "\n",
    "# first run for 2 epochs should be not great fit\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                batch_size=256,\n",
    "                epochs=2,\n",
    "                shuffle=True)\n",
    "\n",
    "x_run = x\n",
    "x1 = x\n",
    "ae_out_1 = autoencoder.predict(x_run)\n",
    "x1[missing_mask] = ae_out_1[missing_mask]\n",
    "\n",
    "print('MSE after 2 epochs:')\n",
    "print(((x1[orig_present] - gene_exp.as_matrix()[:, 1:][orig_present]) ** 2).mean(axis=None))\n",
    "print('')\n",
    "\n",
    "# now run for 18 more\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                batch_size=256,\n",
    "                epochs=18,\n",
    "                shuffle=True)\n",
    "\n",
    "x2 = x\n",
    "ae_out_2 = autoencoder.predict(x_run)\n",
    "x2[missing_mask] = ae_out_2[missing_mask]\n",
    "\n",
    "print('MSE after 20 epochs:')\n",
    "print(((x1[orig_present] - gene_exp.as_matrix()[:, 1:][orig_present]) ** 2).mean(axis=None))\n",
    "print('')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
